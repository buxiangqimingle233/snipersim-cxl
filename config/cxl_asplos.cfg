# Configuration file for Xeon X5550 Gainestown
# See http://en.wikipedia.org/wiki/Gainestown_(microprocessor)#Gainestown
# and http://ark.intel.com/products/37106

#include nehalem
[general]
total_cores = 96
# total_cores = 1

[perf_model/core]
frequency = 2.66

[perf_model/l3_cache]
perfect = false
cache_block_size = 64
cache_size = 8192
associativity = 16
address_hash = mask
replacement_policy = lru
data_access_time = 30 # 35 cycles total according to membench, +L1+L2 tag times
tags_access_time = 10
perf_model_type = parallel
writethrough = 0
shared_cores = 4

[perf_model/dram_directory]
# total_entries = number of entries per directory controller.
total_entries = 1048576
associativity = 16
directory_type = full_map

[traceinput]
numa_balance_strategy = 2           # Specifies how to allocate pages across numa nodes, 
                                    # 0 for "random allocation", 1 for "interleaving", 2 for "local memory first"
bloom_file = bf.bin                 # The mmap file of the bloom filter in the numa balancer

[perf_model/dram]
# -1 means that we have a number of distributed DRAM controllers (4 in this case)
num_controllers = -1
controllers_interleaving = 4

# DRAM access latency in nanoseconds. Should not include L1-LLC tag access time, directory access time (14 cycles = 5.2 ns),
# or network time [(cache line size + 2*{overhead=40}) / network bandwidth = 18 ns]
# Membench says 175 cycles @ 2.66 GHz = 66 ns total
type = readwrite                           # DRAM performance model type: "constant" or a "normal" distribution for 
                                        # a homogeneous memory model, "hybrid" for a hetrogeneous memory model

latency = 66
# local_latency = 100                            # In nanoseconds
# remote_latency = 150                           # In nanoseconds
# local_capacity = 1232896                       # In bytes, 1 GB
# remote_capacity = 616448                       # In bytes, 512 MB, remote memory capacity a dummy metric and 
#                                                # does not take any effects in the current version
per_controller_bandwidth = 7.6                 # In GB/s, as measured by core_validation-dram
chips_per_dimm = 8
dimms_per_controller = 4

[perf_model/dram/readwrite]
shared = false

[perf_model/cxl]
enabled = true
# cxl_mem_roundtrip = 180                 # In nanoseconds, cxl-link + device, should exclude time in normal memory path
# cxl_cache_roundtrip = 400               # In nanoseconds, cxl-link*2 + device*1
cxl_mem_roundtrip = 0
cxl_cache_roundtrip = 0
# cxl_cache_roundtrip = 1

[network]
memory_model_1 = bus
memory_model_2 = bus

[network/bus]
bandwidth = 25.6 # in GB/s. Actually, it's 12.8 GB/s per direction and per connected chip pair
ignore_local_traffic = true # Memory controllers are on-chip, so traffic from core0 to dram0 does not use the QPI links
