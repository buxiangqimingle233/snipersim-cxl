# Common config file for Nehalem core

[general]
enable_icache_modeling = true
total_cores = 64


[perf_model/core]
logical_cpus = 1 # number of SMT threads per core
type = rob
core_model = nehalem # Only have this model 


[perf_model/core/rob_timer]  
commit_width = 8   # Commit bandwidth (instructions per cycle), per SMT thread
in_order = false
issue_contention = true
issue_memops_at_issue = true  # Issue memops to the memory hierarchy at issue time (false = before dispatch)
mlp_histogram = false
outstanding_loads = 64
outstanding_stores = 60
rob_repartition = true
rs_entries = 97
simultaneous_issue = true
store_to_load_forwarding = true
address_disambiguation = true   # Allow loads to bypass preceding stores with an unknown address
perfect_depend_cache_miss = false
m_trace_dependent_miss = false

[perf_model/core/interval_timer]
dispatch_width = 4
window_size = 224
num_outstanding_loadstores = 24 #?

[perf_model/sync]
reschedule_cost = 1000

[caching_protocol]
type = parametric_dram_directory_msi

[perf_model/branch_predictor]
type = pentium_m   #TAGE?
mispredict_penalty=8 # Reflects just the front-end portion (approx) of the penalty for Interval Simulation

[perf_model/tlb]
penalty = 30          # Page walk penalty in cycles

[perf_model/itlb]
size = 128            # Number of I-TLB entries
associativity = 4     # I-TLB associativity

[perf_model/dtlb]
size = 64             # Number of D-TLB entries
associativity = 4     # D-TLB associativity

[perf_model/stlb]
size = 512            # Number of second-level TLB entries
associativity = 4     # S-TLB associativity

[perf_model/cache]
levels = 3

[perf_model/l1_icache]
perfect = false
cache_size = 32
associativity = 4
address_hash = mask
replacement_policy = lru
data_access_time = 2
tags_access_time = 1
perf_model_type = parallel
writethrough = 0
shared_cores = 1
prefetcher = simple
outstanding_misses = 24

[perf_model/l1_icache/prefetcher]
prefetch_on_prefetch_hit = false # Do prefetches only on miss (false), or also on hits to lines brought in by the prefetcher (true)

[perf_model/l1_icache/prefetcher/simple]
flows = 16
flows_per_core = false # true = <flows> per core, false = <flows> shared by all cores
num_prefetches = 4
stop_at_page_boundary = true


[perf_model/l1_dcache]
perfect = false
cache_size = 32
associativity = 8
address_hash = mask
replacement_policy = lru
data_access_time = 4
tags_access_time = 1
perf_model_type = parallel
writethrough = 0
shared_cores = 1
prefetcher = simple
outstanding_misses = 24


[perf_model/l1_dcache/prefetcher]
prefetch_on_prefetch_hit = false # Do prefetches only on miss (false), or also on hits to lines brought in by the prefetcher (true)

[perf_model/l1_dcache/prefetcher/simple]
flows = 16
flows_per_core = false # true = <flows> per core, false = <flows> shared by all cores
num_prefetches = 4
stop_at_page_boundary = true


[perf_model/l2_cache]
perfect = false
cache_size = 256
associativity = 8
address_hash = mask
replacement_policy = lru
data_access_time = 8 # 8.something according to membench, -1 cycle L1 tag access time
# http://www.realworldtech.com/page.cfm?ArticleID=RWT040208182719&p=7
tags_access_time = 3
# Total neighbor L1/L2 access time is around 40/70 cycles (60-70 when it's coming out of L1)
writeback_time = 50 # L3 hit time will be added
perf_model_type = parallel
writethrough = 0
shared_cores = 1
prefetcher = simple

[perf_model/l2_cache/prefetcher]
prefetch_on_prefetch_hit = false # Do prefetches only on miss (false), or also on hits to lines brought in by the prefetcher (true)

[perf_model/l2_cache/prefetcher/simple]
flows = 16
flows_per_core = false # true = <flows> per core, false = <flows> shared by all cores
num_prefetches = 4
stop_at_page_boundary = true


[perf_model/l3_cache]
cache_block_size = 64
address_hash = mask
dvfs_domain = global # L1 and L2 run at core frequency (default), L3 is system frequency
prefetcher = ghb
writeback_time = 0

[perf_model/l3_cache/prefetcher]
prefetch_on_prefetch_hit = false # Do prefetches only on miss (false), or also on hits to lines brought in by the prefetcher (true)

[perf_model/l3_cache/prefetcher/ghb]
width = 2
depth = 2
ghb_size = 512
ghb_table_size = 512



[clock_skew_minimization]
scheme = barrier

[clock_skew_minimization/barrier]
quantum = 100

[dvfs]
transition_latency = 2000 # In ns, "under 2 microseconds" according to http://download.intel.com/design/intarch/papers/323671.pdf (page 8)

[dvfs/simple]
cores_per_socket = 1

[power]
vdd = 1.2 # Volts
technology_node = 22 # nm


[perf_model/core]
frequency = 3.2

[perf_model/l3_cache]
perfect = false
cache_block_size = 64
cache_size = 8192
associativity = 16
address_hash = mask
replacement_policy = lru
data_access_time = 30 # 35 cycles total according to membench, +L1+L2 tag times
tags_access_time = 10
perf_model_type = parallel
writethrough = 0
shared_cores = 4

[perf_model/dram_directory]
# total_entries = number of entries per directory controller.
total_entries = 1048576
associativity = 16
directory_type = full_map

[perf_model/dram]
# -1 means that we have a number of distributed DRAM controllers (4 in this case)
num_controllers = -1
controllers_interleaving = 4
# DRAM access latency in nanoseconds. Should not include L1-LLC tag access time, directory access time (14 cycles = 5.2 ns),
# or network time [(cache line size + 2*{overhead=40}) / network bandwidth = 18 ns]
# Membench says 175 cycles @ 2.66 GHz = 66 ns total
# latency = 80
latency = 800
per_controller_bandwidth = 19.2              # In GB/s, as measured by core_validation-dram
chips_per_dimm = 8
dimms_per_controller = 2

[network]
memory_model_1 = bus
memory_model_2 = bus

[network/bus]
bandwidth = 51.2 # in GB/s. Actually, it's 12.8 GB/s per direction and per connected chip pair
ignore_local_traffic = true # Memory controllers are on-chip, so traffic from core0 to dram0 does not use the QPI links

